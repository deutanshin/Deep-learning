{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "DDJwQPZcupab",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Assignment 1-4: Fully-Connected Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 845,
     "status": "ok",
     "timestamp": 1601234373609,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "VyQblYp0nEZq",
    "new_sheet": false,
    "outputId": "8b3b737f-6cc9-4ddc-ac19-4232065c2481",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "ynKS05gJ4iBo",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "fN1SShPR4lJV",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Setup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "VUCKw4Tl1ddj",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "from utils import reset_seed, Solver\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 5271,
     "status": "ok",
     "timestamp": 1601234378126,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "V2mFlFmQ1ddm",
    "new_sheet": false,
    "outputId": "6b1a14a3-421a-4bf2-e424-1ae15b343287",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "reset_seed(0)\n",
    "data_dict = utils.preprocess_cifar10(cuda=False, dtype=torch.float64)\n",
    "print('Train data shape: ', data_dict['X_train'].shape)\n",
    "print('Train labels shape: ', data_dict['y_train'].shape)\n",
    "print('Validation data shape: ', data_dict['X_val'].shape)\n",
    "print('Validation labels shape: ', data_dict['y_val'].shape)\n",
    "print('Test data shape: ', data_dict['X_test'].shape)\n",
    "print('Test labels shape: ', data_dict['y_test'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "ZeH0OvuEe1CN",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Fully-connected neural networks\n",
    "이전 과제에서는 CIFAR-10에 대해 fully-connected two-layer neural network를 구현했습니다.  \n",
    "하지만 이전 코드들은 loss와 gradient를 하나의 함수에서 계산했기 때문에 모듈성이 높지 않습니다. 이는 더 큰 모델로 갈수록 비효율적인 구현이라고 할 수 있겠습니다.  \n",
    "\n",
    "이상적으로는 모듈식(modular) 설계로 네트워크를 만드는 것이 좋습니다.  \n",
    "서로 다른 layer 타입을 각각 독립적으로 구현한 뒤 이를 조합해 다양한 아키텍처의 모델을 구성할 수 있어야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "3Qiu9_4pe1CP",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "여기서는 fully-connected networks를 더 모듈식 접근 방식으로 구현합니다.  \n",
    "이를 위해 각 layer 구현체에서 `forward`와 `backward` 함수를 구현합니다.  \n",
    "\n",
    "`forward` 함수는 입력, weights, 다른 파라미터들을 받아서 출력값(out)과 backward pass에 필요한 데이터를 저장하는 `cache` 객체를 함께 반환합니다.\n",
    "\n",
    "```python\n",
    "def forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "backward pass는 upstream derivatives와 `cache` 객체를 입력으로 받아, 입력과 weights에 대한 gradients를 반환합니다.    \n",
    "수업 때 배운 backpropagation을 잘 기억해보세요.\n",
    "\n",
    "```python\n",
    "def backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive dout (derivative of loss with respect to outputs) and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "이와 같은 방식으로 여러 layer들을 구현하면 서로 조합해 다양한 아키텍처의 classifier를 손쉽게 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "JB7Eu3qJ9xnm",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Linear (fully-connected) layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "bRdnxsvZunFu",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "각 layer를 구현할 때는 `forward`와 `backward` 두 개의 static method를 가진 class를 정의합니다.  \n",
    "class는 `fully_connected_layers.py`에 제공되어 있으며 여기서 `forward`와 `backward` 메서드를 모두 구현해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "0NNv3l-ne1Cb",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## 구현 하기: Linear layer (forward)\n",
    "`fully_connected_layers.py`에서 `Linear.forward` 메서드를 구현하세요.  \n",
    "구현을 마친 후 다음 코드 셀을 실행할 때 오차가 `1e-7` 이하로 나와야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 5239,
     "status": "ok",
     "timestamp": 1601234378127,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "sjq2Sq4Ze1Cc",
    "new_sheet": false,
    "outputId": "4a32ddfb-49aa-4d56-b73b-89b96c67479a",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from fully_connected_networks import Linear\n",
    "\n",
    "# Test the Linear.forward function\n",
    "num_inputs = 2\n",
    "input_shape = torch.tensor((4, 5, 6))\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * torch.prod(input_shape)\n",
    "weight_size = output_dim * torch.prod(input_shape)\n",
    "\n",
    "x = torch.linspace(-0.1, 0.5, steps=input_size, dtype=torch.float64, device='cpu')\n",
    "w = torch.linspace(-0.2, 0.3, steps=weight_size, dtype=torch.float64, device='cpu')\n",
    "b = torch.linspace(-0.3, 0.1, steps=output_dim, dtype=torch.float64, device='cpu')\n",
    "x = x.reshape(num_inputs, *input_shape)\n",
    "w = w.reshape(torch.prod(input_shape), output_dim)\n",
    "\n",
    "out, _ = Linear.forward(x, w, b)\n",
    "correct_out = torch.tensor([[1.49834984, 1.70660150, 1.91485316],\n",
    "                            [3.25553226, 3.51413301, 3.77273372]]\n",
    "                            ).double()\n",
    "\n",
    "print('Testing Linear.forward function:')\n",
    "print('difference: ', utils.rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "4mxIDo46e1Cf",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## 구현 하기: Linear layer (backward)\n",
    "마찬가지로 `Linear.backward` 메서드를 구현하세요.  \n",
    "구현을 마친 후 다음 코드 셀을 실행할 때 모든 derivation의 오차가 `1e-7` 이하로 나와야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 5223,
     "status": "ok",
     "timestamp": 1601234378128,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "ts85gmote1Cg",
    "new_sheet": false,
    "outputId": "1bc280a6-9ef1-4364-d072-8d962f81832c",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from fully_connected_networks import Linear\n",
    "\n",
    "# Test the Linear.backward function\n",
    "reset_seed(0)\n",
    "x = torch.randn(10, 2, 3, dtype=torch.float64, device='cpu')\n",
    "w = torch.randn(6, 5, dtype=torch.float64, device='cpu')\n",
    "b = torch.randn(5, dtype=torch.float64, device='cpu')\n",
    "dout = torch.randn(10, 5, dtype=torch.float64, device='cpu')\n",
    "\n",
    "dx_num = utils.compute_numeric_gradient(lambda x: Linear.forward(x, w, b)[0], x, dout)\n",
    "dw_num = utils.compute_numeric_gradient(lambda w: Linear.forward(x, w, b)[0], w, dout)\n",
    "db_num = utils.compute_numeric_gradient(lambda b: Linear.forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = Linear.forward(x, w, b)\n",
    "dx, dw, db = Linear.backward(dout, cache)\n",
    "\n",
    "# The error should be around e-10 or less\n",
    "print('Testing Linear.backward function:')\n",
    "print('dx error: ', utils.rel_error(dx_num, dx))\n",
    "print('dw error: ', utils.rel_error(dw_num, dw))\n",
    "print('db error: ', utils.rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "bdIqQzqiJQE6",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# ReLU activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "n2DyqL4Ae1Cl",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## 구현 하기 ReLU activation (forward)\n",
    "`ReLU.forward` 함수에서 ReLU activation function의 forward pass를 구현하세요.  \n",
    "이때 입력 tensor를 in-place 연산으로 변경해서는 안 됩니다.\n",
    "\n",
    "구현을 마친 뒤 아래 코드를 실행해 ReLU forward pass를 테스트하세요.  \n",
    "오차는 `1e-7` 이하이어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 5207,
     "status": "ok",
     "timestamp": 1601234378129,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "QblpieUJe1Cm",
    "new_sheet": false,
    "outputId": "3e7cf951-3c8f-44eb-b5da-ba72042d5184",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from fully_connected_networks import ReLU\n",
    "\n",
    "reset_seed(0)\n",
    "x = torch.linspace(-0.5, 0.5, steps=12, dtype=torch.float64, device='cpu')\n",
    "x = x.reshape(3, 4)\n",
    "\n",
    "out, _ = ReLU.forward(x)\n",
    "correct_out = torch.tensor([[ 0.,          0.,          0.,          0.,        ],\n",
    "                            [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                            [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]],\n",
    "                            dtype=torch.float64,\n",
    "                            device='cpu')\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing ReLU.forward function:')\n",
    "print('difference: ', utils.rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "3bSInb7xe1Cq",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## 구현 하기: ReLU activation (backward)\n",
    "이제 ReLU activation function의 backward pass를 구현하세요.\n",
    "이번에도 입력 tensor를 in-place 연산으로 변경하면 안 됩니다.\n",
    "\n",
    "구현을 마친 뒤 아래 코드를 실행해 `ReLU.backward`를 테스트하세요.  \n",
    "오차는 `1e-8` 이하이어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 5190,
     "status": "ok",
     "timestamp": 1601234378129,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "odiV48zBe1Cr",
    "new_sheet": false,
    "outputId": "710d6e24-1422-4684-92f2-b40f3f68cbd7",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from fully_connected_networks import ReLU\n",
    "\n",
    "reset_seed(0)\n",
    "x = torch.randn(10, 10, dtype=torch.float64, device='cpu')\n",
    "dout = torch.randn(*x.shape, dtype=torch.float64, device='cpu')\n",
    "\n",
    "dx_num = utils.compute_numeric_gradient(lambda x: ReLU.forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = ReLU.forward(x)\n",
    "dx = ReLU.backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of e-12\n",
    "print('Testing ReLU.backward function:')\n",
    "print('dx error: ', utils.rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "eVTMuUOZe1Cv",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# \"Sandwich\" layers\n",
    "Neural net을 구현할 때 자주 사용되는 layer 패턴들이 있습니다. 예를 들어 linear layer 다음에 ReLU가 바로 오는 경우는 매우 흔합니다.<br>\n",
    "이런 패턴을 쉽게 쓰기 위해 편의(convenience) layer를 정의합니다.\n",
    "\n",
    "이렇게 구현하는 방식은 기존 layer 구현을 조합해 새로운 layer를 만들 수 있음을 보여줍니다. 이는 딥러닝 코드를 모듈식으로 구성하는 데 유용한 방법입니다.\n",
    "\n",
    "지금은 `Linear_ReLU`의 `forward`, `backward` 함수를 살펴보고, 아래 코드 셀을 실행해 backward pass에 대해 numeric gradient check를 수행하세요.<br>\n",
    "아래 코드를 실행해 `Linear_ReLU` layer 구현을 numeric gradient checking으로 테스트해 보세요.  \n",
    "오차는 `1e-8` 이하이어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 5173,
     "status": "ok",
     "timestamp": 1601234378129,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "-gaY5YfAe1Cw",
    "new_sheet": false,
    "outputId": "7370db9d-80f6-4c3d-9976-b67ad6a2ceff",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from fully_connected_networks import Linear_ReLU\n",
    "\n",
    "reset_seed(0)\n",
    "x = torch.randn(2, 3, 4, dtype=torch.float64, device='cpu')\n",
    "w = torch.randn(12, 10, dtype=torch.float64, device='cpu')\n",
    "b = torch.randn(10, dtype=torch.float64, device='cpu')\n",
    "dout = torch.randn(2, 10, dtype=torch.float64, device='cpu')\n",
    "\n",
    "out, cache = Linear_ReLU.forward(x, w, b)\n",
    "dx, dw, db = Linear_ReLU.backward(dout, cache)\n",
    "\n",
    "dx_num = utils.compute_numeric_gradient(lambda x: Linear_ReLU.forward(x, w, b)[0], x, dout)\n",
    "dw_num = utils.compute_numeric_gradient(lambda w: Linear_ReLU.forward(x, w, b)[0], w, dout)\n",
    "db_num = utils.compute_numeric_gradient(lambda b: Linear_ReLU.forward(x, w, b)[0], b, dout)\n",
    "\n",
    "# Relative error should be around e-8 or less\n",
    "print('Testing Linear_ReLU.forward and Linear_ReLU.backward:')\n",
    "print('dx error: ', utils.rel_error(dx_num, dx))\n",
    "print('dw error: ', utils.rel_error(dw_num, dw))\n",
    "print('db error: ', utils.rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "qq7-cyfQe1C4",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# 구현 하기: Two-layer network\n",
    "이전 구현에서는 두 개의 layer를 하나의 큰 class에 모두 구현했습니다.  \n",
    "이제 필요한 layer들을 모듈식으로 구현했으니 이걸 사용해 two-layer network를 다시 구현하겠습니다.\n",
    "\n",
    "`TwoLayerNet` class의 구현을 완성하세요.  \n",
    "two-layer net의 forward와 backward pass를 모두 구현했다면, 아래 코드를 실행해 구현을 테스트하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 7734,
     "status": "ok",
     "timestamp": 1601234380728,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "d3JOcfyze1C5",
    "new_sheet": false,
    "outputId": "2d38bb9e-f8fe-4a58-dc79-f817bab4949e",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from fully_connected_networks import TwoLayerNet, softmax_loss\n",
    "\n",
    "reset_seed(0)\n",
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = torch.randn(N, D, dtype=torch.float64, device='cpu')\n",
    "y = torch.randint(C, size=(N,), dtype=torch.int64, device='cpu')\n",
    "\n",
    "std = 1e-3\n",
    "model = TwoLayerNet(\n",
    "          input_dim=D,\n",
    "          hidden_dim=H,\n",
    "          num_classes=C,\n",
    "          weight_scale=std,\n",
    "          dtype=torch.float64,\n",
    "          device='cpu'\n",
    "        )\n",
    "\n",
    "print('Testing initialization ... ')\n",
    "W1_std = torch.abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = torch.abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert torch.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert torch.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "print('Testing test-time forward pass ... ')\n",
    "model.params['W1'] = torch.linspace(-0.7, 0.3, steps=D * H, dtype=torch.float64, device='cpu').reshape(D, H)\n",
    "model.params['b1'] = torch.linspace(-0.1, 0.9, steps=H, dtype=torch.float64, device='cpu')\n",
    "model.params['W2'] = torch.linspace(-0.3, 0.4, steps=H * C, dtype=torch.float64, device='cpu').reshape(H, C)\n",
    "model.params['b2'] = torch.linspace(-0.9, 0.1, steps=C, dtype=torch.float64, device='cpu')\n",
    "X = torch.linspace(-5.5, 4.5, steps=N * D, dtype=torch.float64, device='cpu').reshape(D, N).t()\n",
    "scores = model.loss(X)\n",
    "correct_scores = torch.tensor(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]],\n",
    "    dtype=torch.float64, device='cpu')\n",
    "scores_diff = torch.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print('Testing training loss (no regularization)')\n",
    "y = torch.tensor([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 49.719461034881775\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
    "\n",
    "# Errors should be around e-6 or less\n",
    "for reg in [0.0, 0.7]:\n",
    "  print('Running numeric gradient check with reg = ', reg)\n",
    "  model.reg = reg\n",
    "  loss, grads = model.loss(X, y)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = utils.compute_numeric_gradient(f, model.params[name])\n",
    "    print('%s relative error: %.2e' % (name, utils.rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "q1Odj9XQe1C9",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# 구현 하기: Solver\n",
    "이전 구현에서는 모델 학습 로직이 모델 class 내부에 함께 들어 있었습니다.  \n",
    "그런데 실제로 구현할 때는 학습과 모델 정의를 따로 분리해놓는게 좋습니다.<br>\n",
    "이번 구현에서는 더 모듈식 설계를 따르기 위해 모델 학습 로직을 별도의 class로 분리했습니다.\n",
    "\n",
    "이렇게 학습을 해주는 class를 사람들은 trainer 혹은 solver라고 많이들 부릅니다.\n",
    "\n",
    "`Solver` class를 사용해 `TwoLayerNet`을 학습시키는 solver 인스턴스를 만들고, validation set에서 최소 50% 성능을 달성하세요. \n",
    "`Solver` class의 사용 방법을 숙지하고 `create_solver_instance` 함수를 작성하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 28150,
     "status": "ok",
     "timestamp": 1601234401182,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "6unJrOule1C_",
    "new_sheet": false,
    "outputId": "ca6ccbf0-25a9-4545-e07c-5ee919c9a7e5",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from fully_connected_networks import create_solver_instance\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "# Create a solver instance that achieves 50% performance on the validation set\n",
    "solver = create_solver_instance(data_dict=data_dict, dtype=torch.float64, device='cpu')\n",
    "solver.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 28590,
     "status": "ok",
     "timestamp": 1601234401636,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "gSSy7LTde1DE",
    "new_sheet": false,
    "outputId": "c9eba350-22f4-41c1-caa8-93383663c870",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to visualize training loss and train / val accuracy\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(solver.val_acc_history, '-o', label='val')\n",
    "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "oUwvMomE31Mh",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "모델의 성능이 잘 나온다면 아래 코드 셀을 실행해 모델을 저장할 수 있습니다.  \n",
    "이후 저장된 모델을 다시 불러와 validation 데이터셋에서 검증할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 28847,
     "status": "ok",
     "timestamp": 1601234401902,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "AfE_2VVK31fa",
    "new_sheet": false,
    "outputId": "20e20bf5-837b-4851-ed92-ef9fe0160eeb",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "path = 'best_two_layer_net.pth'\n",
    "solver.model.save(path)\n",
    "\n",
    "# Create a new instance\n",
    "from fully_connected_networks import create_solver_instance\n",
    "reset_seed(0)\n",
    "\n",
    "solver = create_solver_instance(data_dict=data_dict, dtype=torch.float64, device='cpu')\n",
    "\n",
    "# Load model\n",
    "solver.model.load(path, dtype=torch.float64, device='cpu')\n",
    "\n",
    "# Evaluate on validation set\n",
    "accuracy = solver.check_accuracy(solver.X_val, solver.y_val)\n",
    "print(f\"Saved model's accuracy on validation is {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "eNyFLT1We1DI",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# 구현 하기: Multilayer (다층) network\n",
    "이제 여러개의 hidden layer를 가진 fully-connected network를 구현합니다.\n",
    "\n",
    "`fully_connected_networks.py`의 `FullyConnectedNet` class를 보고 initialization(초기화), forward pass, backward pass를 구현하세요.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "3abR1_qhe1DK",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Initial loss and gradient check\n",
    "\n",
    "sanity check를 위해 아래 코드를 실행해 초기 loss를 확인하고 regularization을 켠 경우와 끈 경우 모두에 대해 네트워크의 gradient check를 수행하세요.  \n",
    "gradient checking에서는 대부분의 항목에서 오류가 `1e-6`보다 작아야 합니다.  \n",
    "단, `reg=0`에서 `W1`과 `W2`에 대한 체크는 오류가 `1e-5`보다 작아야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 34953,
     "status": "ok",
     "timestamp": 1601234408023,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "1waPtKRDe1DL",
    "new_sheet": false,
    "outputId": "b55b6750-f048-4e5f-97c1-d5b09610b385",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from fully_connected_networks import FullyConnectedNet\n",
    "\n",
    "reset_seed(0)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = torch.randn(N, D, dtype=torch.float64, device='cpu')\n",
    "y = torch.randint(C, size=(N,), dtype=torch.int64, device='cpu')\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print('Running check with reg = ', reg)\n",
    "  model = FullyConnectedNet(\n",
    "        [H1, H2], \n",
    "        input_dim=D,\n",
    "        num_classes=C,\n",
    "        reg=reg,\n",
    "        weight_scale=5e-2, \n",
    "        dtype=torch.float64, \n",
    "        device='cpu'\n",
    "  )\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print('Initial loss: ', loss.item())\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = utils.compute_numeric_gradient(f, model.params[name])\n",
    "    print('%s relative error: %.2e' % (name, utils.rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "-q6aWzNfe1DQ",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "또 다른 sanity check로 50장의 매우 작은 데이터셋에 대해 overfitting이 가능한지 확인해 보세요.  \n",
    "먼저 hidden layer마다 100개의 unit을 가진 three-layer network를 시도합니다.  \n",
    "\n",
    "아래 셀에서 **learning rate**와 **weight initialization scale**을 조정해, 20 epoch 이내에 training accuracy가 100%에 도달하도록 overfit을 만들어 보세요 (validation accuracy가 아니니까 주의하세요).\n",
    "<br>\n",
    "만약 아무리해도 overfitting이 일어나지 않는다면 구현이 잘못되었다고 의심할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 953
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 35371,
     "status": "ok",
     "timestamp": 1601234408445,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "2NccCDJ3e1DR",
    "new_sheet": false,
    "outputId": "f1fbf7a9-ecdc-497e-eaab-9f3d40c8b7fc",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from fully_connected_networks import FullyConnectedNet, get_three_layer_network_params\n",
    "\n",
    "# TODO: Use a three-layer Net to overfit 50 training examples by \n",
    "# tweaking just the learning rate and initialization scale.\n",
    "reset_seed(0)\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data_dict['X_train'][:num_train],\n",
    "  'y_train': data_dict['y_train'][:num_train],\n",
    "  'X_val': data_dict['X_val'],\n",
    "  'y_val': data_dict['y_val'],\n",
    "}\n",
    "\n",
    "# Update parameters in get_three_layer_network_params\n",
    "weight_scale, learning_rate = get_three_layer_network_params()\n",
    "\n",
    "model = FullyConnectedNet([100, 100],\n",
    "              weight_scale=weight_scale, dtype=torch.float32, device='cpu')\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                },\n",
    "                device='cpu',\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "tskjw8VUe1DV",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "이번에는 각 layer에 100개의 unit을 가진 five-layer network를 사용해, 50개의 training 예제를 overfit시켜 보세요.  \n",
    "마찬가지로 **learning rate**와 **weight initialization scale**을 조정해야 하지만, 20 epoch 이내에 training accuracy 100%를 달성할 수 있어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 953
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 36411,
     "status": "ok",
     "timestamp": 1601234409494,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "D5mAWrrPe1Dc",
    "new_sheet": false,
    "outputId": "70c9ed1c-4214-4114-8c3e-593e8b8de64e",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from fully_connected_networks import FullyConnectedNet, get_five_layer_network_params\n",
    "\n",
    "# TODO: Use a five-layer Net to overfit 50 training examples by \n",
    "# tweaking just the learning rate and initialization scale.\n",
    "reset_seed(0)\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data_dict['X_train'][:num_train],\n",
    "  'y_train': data_dict['y_train'][:num_train],\n",
    "  'X_val': data_dict['X_val'],\n",
    "  'y_val': data_dict['y_val'],\n",
    "}\n",
    "\n",
    "\n",
    "# Update parameters in get_three_layer_network_params\n",
    "weight_scale, learning_rate = get_five_layer_network_params()\n",
    "\n",
    "# Run models and solver with parameters\n",
    "model = FullyConnectedNet([100, 100, 100, 100],\n",
    "                weight_scale=weight_scale, dtype=torch.float32, device='cpu')\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                },\n",
    "                device='cpu',\n",
    "         )\n",
    "# Turn off keep_best_params to allow final weights to be saved, instead of best weights on validation set.\n",
    "solver.train(return_best_params=False)\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "3M2JQjj_93RW",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "모델의 성능이 만족스럽다면 overfit된 모델을 저장하세요.  \n",
    "sanity check를 위해 저장한 모델을 다시 training set에서 평가해, 저장된 weights가 성능을 내는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 36393,
     "status": "ok",
     "timestamp": 1601234409495,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "YAnM8V9z938Q",
    "new_sheet": false,
    "outputId": "599233ed-1b62-47a3-c7f8-f6a945eb6607",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Set path \n",
    "path = 'best_overfit_five_layer_net.pth'\n",
    "solver.model.save(path)\n",
    "\n",
    "\n",
    "# Create a new instance  -- Note that hidden dims being different doesn't matter here.\n",
    "model = FullyConnectedNet(hidden_dims=[100, ], dtype=torch.float32, device='cpu')\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                },\n",
    "                device='cpu',\n",
    "         )\n",
    "\n",
    "\n",
    "# Load model\n",
    "solver.model.load(path, dtype=torch.float32, device='cpu')\n",
    "\n",
    "# Evaluate on validation set\n",
    "accuracy = solver.check_accuracy(solver.X_train, solver.y_train)\n",
    "print(f\"Saved model's accuracy on small train is {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "T4eWrnY7e1Di",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Update rules\n",
    "지금까지는 update rule로 기본 stochastic gradient descent(SGD)만 사용했습니다.  \n",
    "하지만 수업 때 배운것과 같이 더 발전된 update rule을 사용하면 모델 학습이 더 쉬워질 수 있습니다.  \n",
    "\n",
    "이제 자주 사용되는 몇 가지 update rule을 구현하고, 이를 vanilla SGD와 비교해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "zBDJqbeVe1Dn",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## 구현 하기: SGD+Momentum\n",
    "우리가 구현할 모든 update rule은 동일한 인터페이스를 가집니다:\n",
    "\n",
    "```python\n",
    "def update(w, dw, config=None):\n",
    "Inputs:\n",
    "  - w: A tensor giving the current weights.\n",
    "  - dw: A tensor of the same shape as w giving the gradient of the\n",
    "    loss with respect to w.\n",
    "  - config: A dictionary containing hyperparameter values such as learning\n",
    "    rate, momentum, etc. If the update rule requires caching values over many\n",
    "    iterations, then config will also hold these cached values.\n",
    "Returns:\n",
    "  - next_w: The next point after the update.\n",
    "  - config: The config dictionary to be passed to the next iteration of the\n",
    "    update rule.\n",
    "NOTE: For most update rules, the default learning rate will probably not\n",
    "perform well; however the default values of the other hyperparameters should\n",
    "work well for a variety of different problems.\n",
    "For efficiency, update rules may perform in-place updates, mutating w and\n",
    "setting next_w equal to w.\n",
    "```\n",
    "\n",
    "`fully_connected_networks.py`에 참고용으로 SGD update rule 구현을 제공했습니다.\n",
    "\n",
    "이제 같은 인터페이스로 SGD+Momentum update rule을 구현하고 아래 코드 셀을 실행하세요. 오차는 `1e-7` 이하이어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 36378,
     "status": "ok",
     "timestamp": 1601234409495,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "RbQrkNo_e1Dp",
    "new_sheet": false,
    "outputId": "aeccd6f4-eb31-461a-9fe9-5806e912f6fd",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from fully_connected_networks import sgd_momentum\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "N, D = 4, 5\n",
    "w = torch.linspace(-0.4, 0.6, steps=N*D, dtype=torch.float64, device='cpu').reshape(N, D)\n",
    "dw = torch.linspace(-0.6, 0.4, steps=N*D, dtype=torch.float64, device='cpu').reshape(N, D)\n",
    "v = torch.linspace(0.6, 0.9, steps=N*D, dtype=torch.float64, device='cpu').reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-3, 'velocity': v}\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "expected_next_w = torch.tensor([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]],\n",
    "   dtype=torch.float64, device='cpu')\n",
    "expected_velocity = torch.tensor([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]],\n",
    "   dtype=torch.float64, device='cpu')\n",
    "\n",
    "# Should see relative errors around e-8 or less\n",
    "print('next_w error: ', utils.rel_error(next_w, expected_next_w))\n",
    "print('velocity error: ', utils.rel_error(expected_velocity, config['velocity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "7QQj73zje1D2",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "이제 SGD와 SGD+Momentum을 사용해서 네트워크를 학습합니다.   \n",
    "Momentum을 사용한 경우 더 학습이 빠르게 되는것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 38894,
     "status": "ok",
     "timestamp": 1601234412015,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "qXdMNC9Ve1D4",
    "new_sheet": false,
    "outputId": "94f7c217-595c-4f25-dbd1-594257766175",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from fully_connected_networks import FullyConnectedNet, sgd, sgd_momentum\n",
    "\n",
    "# TODO: Use a three-layer Net to overfit 50 training examples by \n",
    "# tweaking just the learning rate and initialization scale.\n",
    "reset_seed(0)\n",
    "\n",
    "num_train = 4000\n",
    "small_data = {\n",
    "  'X_train': data_dict['X_train'][:num_train],\n",
    "  'y_train': data_dict['y_train'][:num_train],\n",
    "  'X_val': data_dict['X_val'],\n",
    "  'y_val': data_dict['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "\n",
    "for update_rule_name, update_rule_fn in [('sgd', sgd), ('sgd_momentum', sgd_momentum)]:\n",
    "  print('running with ', update_rule_name)\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2,\n",
    "                            dtype=torch.float32, device='cpu')\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule_fn,\n",
    "                  optim_config={\n",
    "                    'learning_rate': 5e-2,\n",
    "                  },\n",
    "                  print_every=1000,\n",
    "                  verbose=True,\n",
    "                  device='cpu')\n",
    "  solvers[update_rule_name] = solver\n",
    "  solver.train()\n",
    "  print()\n",
    "  \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "for update_rule, solver in solvers.items():\n",
    "  plt.plot(solver.loss_history, 'o', label=\"loss_%s\" % update_rule)\n",
    "plt.legend(loc='lower center', ncol=4)\n",
    "  \n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "for update_rule, solver in solvers.items():\n",
    "  plt.plot(solver.train_acc_history, '-o', label=\"train_acc_%s\" % update_rule)\n",
    "plt.legend(loc='lower center', ncol=4)\n",
    "\n",
    "  \n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "for update_rule, solver in solvers.items():\n",
    "  plt.plot(solver.val_acc_history, '-o', label=\"val_acc_%s\" % update_rule)\n",
    "plt.legend(loc='lower center', ncol=4)\n",
    "\n",
    "plt.gcf().set_size_inches(10, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "wYtKqDdEe1D-",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## 구현 하기: RMSProp\n",
    "\n",
    "`fully_connected_networks.py`의 `rmsprop` 함수에서 RMSProp update rule을 구현하세요.  \n",
    "구현을 마친 뒤 아래 코드를 실행해 RMSProp 구현을 테스트하세요. 오차는 `1e-6` 이하이어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 38890,
     "status": "ok",
     "timestamp": 1601234412016,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "RBBpJhJie1D_",
    "new_sheet": false,
    "outputId": "36a80bcb-2e97-45e5-a516-34df6eed7749",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Test RMSProp implementation\n",
    "from fully_connected_networks import rmsprop\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "N, D = 4, 5\n",
    "w = torch.linspace(-0.4, 0.6, steps=N*D, dtype=torch.float64, device='cpu').reshape(N, D)\n",
    "dw = torch.linspace(-0.6, 0.4, steps=N*D, dtype=torch.float64, device='cpu').reshape(N, D)\n",
    "cache = torch.linspace(0.6, 0.9, steps=N*D, dtype=torch.float64, device='cpu').reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'cache': cache}\n",
    "next_w, _ = rmsprop(w, dw, config=config)\n",
    "\n",
    "expected_next_w = torch.tensor([\n",
    "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
    "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
    "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
    "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]],\n",
    "   dtype=torch.float64, device='cpu')\n",
    "expected_cache = torch.tensor([\n",
    "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
    "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
    "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
    "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]],\n",
    "   dtype=torch.float64, device='cpu')\n",
    "\n",
    "print('next_w error: ', utils.rel_error(expected_next_w, next_w))\n",
    "print('cache error: ', utils.rel_error(expected_cache, config['cache']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "bMdq7WRFDiJw",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## 구현 하기 Adam\n",
    "마찬가지로 Adam 기법을 구현합니다.\n",
    "`next_w`의 error는 `1e-6` 미만, `v`와 `m`의 error는 `1e-8` 미만이어야 합니다.\n",
    "\n",
    "주의 사항: bias correction 까지 구현해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 38885,
     "status": "ok",
     "timestamp": 1601234412016,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "ovUXV51Le1EE",
    "new_sheet": false,
    "outputId": "11c35e53-f9a3-4094-9b0a-485492bf3634",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Adam implementation\n",
    "from fully_connected_networks import adam\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "N, D = 4, 5\n",
    "w = torch.linspace(-0.4, 0.6, steps=N*D, dtype=torch.float64, device='cpu').reshape(N, D)\n",
    "dw = torch.linspace(-0.6, 0.4, steps=N*D, dtype=torch.float64, device='cpu').reshape(N, D)\n",
    "m = torch.linspace(0.6, 0.9, steps=N*D, dtype=torch.float64, device='cpu').reshape(N, D)\n",
    "v = torch.linspace(0.7, 0.5, steps=N*D, dtype=torch.float64, device='cpu').reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n",
    "next_w, _ = adam(w, dw, config=config)\n",
    "\n",
    "expected_next_w = torch.tensor([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]],\n",
    "   dtype=torch.float64, device='cpu')\n",
    "expected_v = torch.tensor([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]],\n",
    "   dtype=torch.float64, device='cpu')\n",
    "expected_m = torch.tensor([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]],\n",
    "   dtype=torch.float64, device='cpu')\n",
    "\n",
    "# You should see relative errors around e-7 or less\n",
    "print('next_w error: ', utils.rel_error(expected_next_w, next_w))\n",
    "print('v error: ', utils.rel_error(expected_v, config['v']))\n",
    "print('m error: ', utils.rel_error(expected_m, config['m']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "1T_qzgxte1EI",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "이제 두 최적화 기법을 통해서 모델을 학습해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 41551,
     "status": "ok",
     "timestamp": 1601234414686,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "6TFopQgre1EJ",
    "new_sheet": false,
    "outputId": "ee61b02f-531e-4b46-88f0-4a82936e5f9c",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Adam implementation\n",
    "from fully_connected_networks import adam, rmsprop, FullyConnectedNet\n",
    "\n",
    "for update_rule_name, update_rule_fn, learning_rate in [('adam', adam, 1e-3), ('rmsprop', rmsprop, 1e-4)]:\n",
    "  print('running with ', update_rule)\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2, device='cpu')\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule_fn,\n",
    "                  optim_config={\n",
    "                    'learning_rate': learning_rate\n",
    "                  },\n",
    "                  print_every=1000,\n",
    "                  verbose=True, device='cpu')\n",
    "  solvers[update_rule_name] = solver\n",
    "  solver.train()\n",
    "  print()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "for update_rule, solver in list(solvers.items()):\n",
    "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "plt.legend(loc='lower center', ncol=4)\n",
    "  \n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "for update_rule, solver in list(solvers.items()):\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "plt.legend(loc='lower center', ncol=4)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "for update_rule, solver in list(solvers.items()):\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "plt.legend(loc='lower center', ncol=4)\n",
    "\n",
    "plt.gcf().set_size_inches(10, 20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "fully_connected_networks.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
