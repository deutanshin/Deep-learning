{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vTOX0d3cmLQf"
   },
   "source": [
    "# Assignment 1-2: Linear Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1027,
     "status": "ok",
     "timestamp": 1600042524722,
     "user": {
      "displayName": "Yunseok Jang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji2utsrQJWXntm3ishdCA23wmdDA4QyRS8UrqQsEQ=s64",
      "userId": "10051210866960976186"
     },
     "user_tz": 240
    },
    "id": "VyQblYp0nEZq",
    "outputId": "1902f06e-d588-4ac5-97e9-299a8b2314f8"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ynKS05gJ4iBo"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fN1SShPR4lJV"
   },
   "source": [
    "## Setup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VUCKw4Tl1ddj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Yv3zQYw5B3s"
   },
   "source": [
    "## Load the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COCx2kM6XB3K"
   },
   "source": [
    "아래 코드는 `utils.data.preprocess` 함수를 통해 데이터셋을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3040,
     "status": "ok",
     "timestamp": 1600042542853,
     "user": {
      "displayName": "Yunseok Jang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji2utsrQJWXntm3ishdCA23wmdDA4QyRS8UrqQsEQ=s64",
      "userId": "10051210866960976186"
     },
     "user_tz": 240
    },
    "id": "r_BhZ_6_XB3K",
    "outputId": "92b4e51e-d0e0-43c7-e1f0-d3a116b1f06d"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "utils.reset_seed(0)\n",
    "data_dict = utils.preprocess_cifar10(cuda=False, dtype=torch.float64)\n",
    "print('Train data shape: ', data_dict['X_train'].shape)\n",
    "print('Train labels shape: ', data_dict['y_train'].shape)\n",
    "print('Validation data shape: ', data_dict['X_val'].shape)\n",
    "print('Validation labels shape: ', data_dict['y_val'].shape)\n",
    "print('Test data shape: ', data_dict['X_test'].shape)\n",
    "print('Test labels shape: ', data_dict['y_test'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Lvdm4fm7iJC"
   },
   "source": [
    "# Softmax cross-entropy Loss 기반 Linear Classifier\n",
    "\n",
    "본 과제에서는 아래와 같은 내용을 다루게 됩니다\n",
    "- Softmax 분류기의 **loss function**을 구현한다.  \n",
    "- **Gradient**의 해석적 표현(analytic)을 완전 벡터화로 구현한다.  \n",
    "- 수치 미분과 비교해 **구현을 검증**한다.  \n",
    "- Test dataset을 사용해 **learning rate**와 **regularization** 강도를 **tuning**한다.  \n",
    "- **SGD**를 사용해 **loss function**을 **최적화**한다.  \n",
    "- 최종적으로 학습된 **weights**를 **시각화**한다.  \n",
    "\n",
    "Note: `.to()`나 `.cuda()`와 같은 메소드를 사용하지 마세요. 이 경우 의도치 않은 에러가 발생할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xTI4qN7S9aTr"
   },
   "source": [
    "### 구현 하기\n",
    "\n",
    "먼저 `softmax_loss_naive` 함수에서 **중첩 loop**를 사용해 naive한 softmax cross-entropy **loss function**을 구현합니다.  \n",
    "구현한 **loss**가 올바른지 간단히 확인(sanity check)하기 위해 아래 코드는 작은 랜덤 **weight** 행렬과 regularization을 사용하지 않은 상태에서 softmax classifier를 실행합니다.<br>\n",
    "이때 출력되는 **loss**는 log(10) ≈ 2.3 근처여야 합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22697,
     "status": "ok",
     "timestamp": 1600042563158,
     "user": {
      "displayName": "Yunseok Jang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji2utsrQJWXntm3ishdCA23wmdDA4QyRS8UrqQsEQ=s64",
      "userId": "10051210866960976186"
     },
     "user_tz": 240
    },
    "id": "Hxzu2uZq9P8P",
    "outputId": "6c68b037-6ed6-4fbd-8dbf-faab64a41396"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from linear_classifier import softmax_loss_naive\n",
    "\n",
    "utils.reset_seed(0)\n",
    "# Generate a random softmax weight tensor and use it to compute the loss.\n",
    "W = 0.0001 * torch.randn(3072, 10, dtype=data_dict['X_val'].dtype, device=data_dict['X_val'].device)\n",
    "\n",
    "X_batch = data_dict['X_val'][:128]\n",
    "y_batch = data_dict['y_val'][:128]\n",
    "\n",
    "loss, _ = softmax_loss_naive(W, X_batch, y_batch, reg=0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to log(10.0).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (math.log(10.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9LbRTXJ39Yp8"
   },
   "source": [
    "### 구현 하기\n",
    "\n",
    "위에서 함수가 반환한 `_grad_`는 현재 전부 0입니다.  \n",
    "`softmax_loss_naive` 함수 안의 TODO 블록을 채워 넣어 softmax cross-entropy loss function의 **gradient**를 유도하고, 그 식을 함수 안에 직접 구현하세요.  \n",
    "기존 코드 흐름 속에 새 코드를 적절히 섞어 넣는 방식이 도움이 됩니다.\n",
    "\n",
    "아래 코드 셀은 구현한 gradient 계산이 맞는지 확인하기 위해 numeric gradient checking을 사용합니다.<br>\n",
    "즉, finite differences로 forward pass의 gradient를 numerical하게 근사하고, 이 값과 여러분이 구현한 analytic gradient를 비교합니다.\n",
    "\n",
    "Gradient 계산을 제대로 구현했으면 아래 실행되는 셀에서 **relative error**가 `1e-5`보다 작게 나와야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24726,
     "status": "ok",
     "timestamp": 1600042565991,
     "user": {
      "displayName": "Yunseok Jang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji2utsrQJWXntm3ishdCA23wmdDA4QyRS8UrqQsEQ=s64",
      "userId": "10051210866960976186"
     },
     "user_tz": 240
    },
    "id": "o3sMha4i9p_V",
    "outputId": "728087f4-d83d-476c-d8b6-57e40378375e"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from linear_classifier import softmax_loss_naive\n",
    "\n",
    "utils.reset_seed(0)\n",
    "W = 0.0001 * torch.randn(3072, 10, dtype=data_dict['X_val'].dtype, device=data_dict['X_val'].device)\n",
    "batch_size = 64\n",
    "X_batch = data_dict['X_val'][:batch_size]\n",
    "y_batch = data_dict['y_val'][:batch_size]\n",
    "\n",
    "# Compute the loss and its gradient at W.\n",
    "# TODO: implement the gradient part of 'softmax_loss_naive' function in \"linear_classifier.py\"\n",
    "_, grad = softmax_loss_naive(W, X_batch, y_batch, reg=0.0)\n",
    "\n",
    "# Numerically compute the gradient along several randomly chosen dimensions, and\n",
    "# compare them with your analytically computed gradient. The numbers should\n",
    "# match almost exactly along all dimensions.\n",
    "f = lambda w: softmax_loss_naive(w, X_batch, y_batch, reg=0.0)[0]\n",
    "grad_numerical = utils.grad_check_sparse(f, W, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lSdsG-L292Ww"
   },
   "source": [
    "아래 코드 셀은 regularization을 켠 상태에서 gradient check를 합니다.<br>\n",
    "마찬가지로 relative errors는 `1e-5` 미만이어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27345,
     "status": "ok",
     "timestamp": 1600042568998,
     "user": {
      "displayName": "Yunseok Jang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji2utsrQJWXntm3ishdCA23wmdDA4QyRS8UrqQsEQ=s64",
      "userId": "10051210866960976186"
     },
     "user_tz": 240
    },
    "id": "bH6lXxVn9xZk",
    "outputId": "9de60bca-a304-42ef-9c71-896c7afd7e63"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from linear_classifier import softmax_loss_naive\n",
    "\n",
    "utils.reset_seed(0)\n",
    "W = 0.0001 * torch.randn(3072, 10, dtype=data_dict['X_val'].dtype, device=data_dict['X_val'].device)\n",
    "batch_size = 64\n",
    "X_batch = data_dict['X_val'][:batch_size]\n",
    "y_batch = data_dict['y_val'][:batch_size]\n",
    "\n",
    "# Compute the loss and its gradient at W.\n",
    "# TODO: check your 'softmax_loss_naive' implementation with different 'reg'\n",
    "_, grad = softmax_loss_naive(W, X_batch, y_batch, reg=1e3) \n",
    "\n",
    "# Numerically compute the gradient along several randomly chosen dimensions, and\n",
    "# compare them with your analytically computed gradient. The numbers should\n",
    "# match almost exactly along all dimensions.\n",
    "f = lambda w: softmax_loss_naive(w, X_batch, y_batch, reg=1e3)[0]\n",
    "grad_numerical = utils.grad_check_sparse(f, W, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sc5Wtu-e-WlI"
   },
   "source": [
    "### 구현 하기\n",
    "\n",
    "이제 **vectorized** 버전 `softmax_loss_vectorized`를 구현합니다.<br>\n",
    "기존에 구현한 naive 버전과 vectorized 버전의 차이점은 **명시적인 loop 없이 구현**을 해야한다는 점입니다.<br>\n",
    "입출력은 앞서 만든 naive 버전과 동일해야 합니다.\n",
    "\n",
    "잘 구현이 되었다면 수치적인 오차는 (결과 값에 차이가 있을수 있으나) `1e-6` 이하여야 합니다.<br>\n",
    "하지만 non-vectorized 버전과 vectorized 버전의 speed는 약 3\\~4배 정도 차이나게 됩니다.<br>\n",
    "GPU에서 실행하는 경우 15\\~20x speed 향상을 기대할 수 있지만 본 과제는 CPU에서 실행하기 때문에 vectorized 구현이 그렇게 차이가 많이 나지는 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27538,
     "status": "ok",
     "timestamp": 1600042569595,
     "user": {
      "displayName": "Yunseok Jang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji2utsrQJWXntm3ishdCA23wmdDA4QyRS8UrqQsEQ=s64",
      "userId": "10051210866960976186"
     },
     "user_tz": 240
    },
    "id": "pBLqLTAGo1Rs",
    "outputId": "89bba281-36e9-4f15-8f30-a0100c48ca57"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from linear_classifier import softmax_loss_naive, softmax_loss_vectorized\n",
    "\n",
    "utils.reset_seed(0)\n",
    "W = 0.0001 * torch.randn(3072, 10, dtype=data_dict['X_val'].dtype, device=data_dict['X_val'].device)\n",
    "reg = 0.05\n",
    "\n",
    "X_batch = data_dict['X_val'][:128]\n",
    "y_batch = data_dict['y_val'][:128]\n",
    "\n",
    "# Run and time the naive version\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_batch, y_batch, reg)\n",
    "toc = time.time()\n",
    "ms_naive = 1000.0 * (toc - tic)\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, ms_naive))\n",
    "\n",
    "# Run and time the vectorized version\n",
    "# TODO: Complete the implementation of softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vec, grad_vec = softmax_loss_vectorized(W, X_batch, y_batch, reg)\n",
    "toc = time.time()\n",
    "ms_vec = 1000.0 * (toc - tic)\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vec, ms_vec))\n",
    "\n",
    "# we use the Frobenius norm to compare the two versions of the gradient.\n",
    "loss_diff = (loss_naive - loss_vec).abs().item()\n",
    "grad_diff = torch.norm(grad_naive - grad_vec, p='fro')\n",
    "print('Loss difference: %.2e' % loss_diff)\n",
    "print('Gradient difference: %.2e' % grad_diff)\n",
    "print('Speedup: %.2fX' % (ms_naive / ms_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uU852IitCtrC"
   },
   "source": [
    "### 구현 하기\n",
    "\n",
    "이제 loss function과 gradient 계산을 vectorized 방식으로 구현했으므로 이를 활용해 선형 분류기(linear classifier)의 학습 파이프라인을 구현할 차례입니다.<br>\n",
    "`linear_classifer.py` 파일 안의 `train_linear_classifier` 함수를 완성하세요.  \n",
    "\n",
    "잘 구현했다면 아래 코드 셀을 실행해 기본 하이퍼파라미터로 선형 분류기(linear classifier)를 학습해 보세요.<br>\n",
    "(참고로 학습이 잘 되지 않는게 정상입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29022,
     "status": "ok",
     "timestamp": 1600042572063,
     "user": {
      "displayName": "Yunseok Jang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji2utsrQJWXntm3ishdCA23wmdDA4QyRS8UrqQsEQ=s64",
      "userId": "10051210866960976186"
     },
     "user_tz": 240
    },
    "id": "QaEZkCe3-kOu",
    "outputId": "d0bb5f29-d597-4185-855e-beba0c8c325c"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from linear_classifier import softmax_loss_vectorized, train_linear_classifier\n",
    "\n",
    "utils.reset_seed(0)\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "W, loss_hist = train_linear_classifier(softmax_loss_vectorized, None, \n",
    "                                       data_dict['X_train'], \n",
    "                                       data_dict['y_train'], \n",
    "                                       learning_rate=1e-10, reg=2.5e4,\n",
    "                                       num_iters=1500, verbose=True)\n",
    "\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DRdfknKsE6F2"
   },
   "source": [
    "### 구현 하기\n",
    "\n",
    "학습 코드는 작성했으니 이제 prediction 단계를 구현합니다.\n",
    "학습된 모델을 training set과 validation set에서 평가해 보세요.  \n",
    "위에서 학습한 모델은 제대로 학습이 되지 않았기 때문에 결과로 나오는 **validation accuracy**는 10% 미만이어야 합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28692,
     "status": "ok",
     "timestamp": 1600042572491,
     "user": {
      "displayName": "Yunseok Jang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji2utsrQJWXntm3ishdCA23wmdDA4QyRS8UrqQsEQ=s64",
      "userId": "10051210866960976186"
     },
     "user_tz": 240
    },
    "id": "YfToPzce_OBH",
    "outputId": "be83e513-152e-4a60-e7f9-37e590ea6d3c"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from linear_classifier import predict_linear_classifier\n",
    "\n",
    "# fix random seed before we perform this operation\n",
    "utils.reset_seed(0)\n",
    "\n",
    "# evaluate the performance on both the training and validation set\n",
    "# YOUR_TURN: Implement how to make a prediction with the trained weight \n",
    "#            in 'predict_linear_classifier'\n",
    "y_train_pred = predict_linear_classifier(W, data_dict['X_train'])\n",
    "train_acc = 100.0 * (data_dict['y_train'] == y_train_pred).double().mean().item()\n",
    "print('Training accuracy: %.2f%%' % train_acc)\n",
    "\n",
    "y_val_pred = predict_linear_classifier(W, data_dict['X_val'])\n",
    "val_acc = 100.0 * (data_dict['y_val'] == y_val_pred).double().mean().item()\n",
    "print('Validation accuracy: %.2f%%' % val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "taNmjt2wGJQr"
   },
   "source": [
    "### 구현 하기 & 실험 하기\n",
    "참고로 validation accuracy가 10%인 경우는 랜덤으로 찍는 것과 동일한 결과입니다...\n",
    "\n",
    "그럼 이제 최적의 하이퍼파라미터(hyperparameter)를 찾아 좀 더 좋은 모델 성능을 기록해봅시다.<br>\n",
    "아래 코드 셀은 validation dataset을 사용해서 hyperparameter tuning을 진행합니다.<br>\n",
    "여기서 다양한 하이퍼파라미터를 탐색해보세요. 특히 regularization strength와 learning rate가 중요합니다.<br>\n",
    "\n",
    "**본 과제에서는 `validation accuracy`가 37%를 넘는 결과를 한 번이라도 만들어야 100점으로 인정합니다.** 다양한 하이퍼파라미터 탐색을 진행해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 76733,
     "status": "ok",
     "timestamp": 1600042621103,
     "user": {
      "displayName": "Yunseok Jang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji2utsrQJWXntm3ishdCA23wmdDA4QyRS8UrqQsEQ=s64",
      "userId": "10051210866960976186"
     },
     "user_tz": 240
    },
    "id": "oVMsJ9Ti_Ude",
    "outputId": "2f48ea8d-b1da-4e84-ad60-6ba99cd564a7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "from linear_classifier import Softmax, softmax_get_search_params, test_one_param_set\n",
    "\n",
    "# TODO: find the best learning_rates and regularization_strengths combination in 'softmax_get_search_params'\n",
    "learning_rates, regularization_strengths = softmax_get_search_params()\n",
    "num_models = len(learning_rates) * len(regularization_strengths)\n",
    "\n",
    "i = 0\n",
    "# As before, store your cross-validation results in this dictionary.\n",
    "# The keys should be tuples of (learning_rate, regularization_strength) and\n",
    "# the values should be tuples (train_acc, val_acc)\n",
    "results = {}\n",
    "best_val = -1.0   # The highest validation accuracy that we have seen so far.\n",
    "best_softmax_model = None # The Softmax object that achieved the highest validation rate.\n",
    "num_iters = 2000 # number of iterations\n",
    "\n",
    "for lr in learning_rates:\n",
    "  for reg in regularization_strengths:\n",
    "    i += 1\n",
    "    print('Training Softmax %d / %d with learning_rate=%e and reg=%e'\n",
    "          % (i, num_models, lr, reg))\n",
    "    \n",
    "    utils.reset_seed(0)\n",
    "    cand_softmax_model, cand_train_acc, cand_val_acc = test_one_param_set(Softmax(), data_dict, lr, reg, num_iters)\n",
    "\n",
    "    if cand_val_acc > best_val:\n",
    "      best_val = cand_val_acc\n",
    "      best_softmax_model = cand_softmax_model # save the classifier\n",
    "    results[(lr, reg)] = (cand_train_acc, cand_val_acc)\n",
    "\n",
    "\n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "  train_acc, val_acc = results[(lr, reg)]\n",
    "  print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "         lr, reg, train_acc, val_acc))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 하이퍼파라미터 탐색 결과를 시각화합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GsmqO2p2XYhi"
   },
   "source": [
    "아래 코드는 학습한 최고의 모델을 테스트 데이터셋에 테스트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = best_softmax_model.predict(data_dict['X_test'])\n",
    "test_accuracy = torch.mean((data_dict['y_test'] == y_test_pred).double())\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드 셀은 학습한 linear classifier의 weight를 시각화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = best_softmax_model.W\n",
    "w = w.reshape(3, 32, 32, 10)\n",
    "w = w.transpose(0, 2).transpose(1, 0)\n",
    "\n",
    "w_min, w_max = torch.min(w), torch.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "\n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.type(torch.uint8).cpu())\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "FrfeHl_-m4V-",
    "Q7ymI0aZ2W1b",
    "ynKS05gJ4iBo",
    "fN1SShPR4lJV",
    "-Yv3zQYw5B3s",
    "0Lvdm4fm7iJC",
    "DkuwyMY27RxS"
   ],
   "name": "linear_classifier.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
