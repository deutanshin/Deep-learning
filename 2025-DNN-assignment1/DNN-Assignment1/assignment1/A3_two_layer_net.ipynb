{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNmLmqrJAXXp"
   },
   "source": [
    "# Assignment 1-3: Two Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QqEfH2Rpn9J3"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KtMy3qeipNK3"
   },
   "source": [
    "## Setup codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3EvIZ0uAOVN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import random\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hbe3wUpVAjma"
   },
   "source": [
    "# Implementing a Neural Network\n",
    "본 과제에서는 CIFAR-10 데이터셋에 대해 fully-connected 층으로 구성된 **neural network**를 만들어 classification을 수행해봅니다.\n",
    "\n",
    "Network는 이전에 구현했던 softmax cross-entropy loss function과 L2 regularization으로 학습합니다.  \n",
    "그리고 첫 번째 fully-connected layer 뒤에는 **ReLU** activation function으로 사용합니다.\n",
    "\n",
    "정리하면 모델 구조는 다음과 같습니다<br>\n",
    "- input → fully connected layer → ReLU → fully connected layer → softmax\n",
    "\n",
    "두 번째 fully-connected layer의 출력은 각 클래스에 대한 scores입니다. 따라서 ReLU를 사용하지 않습니다.<br>\n",
    "**Note**: `W`에 대한 regularization을 구현할 때 **regularization term에 1/2를 곱하지 마세요**(계수를 사용하지 말라는 의미입니다)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lJqim3P1qZgv"
   },
   "source": [
    "## Play with a toy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5T-4Phbd9GvI"
   },
   "source": [
    "우리가 구현할 network의 입력은 배치 크기 `N`(`num_inputs`)과 `D`(`input_size`) dimension(차원)을 가진 벡터들입니다.  \n",
    "hidden layer는 `H`개의 hidden units(`hidden_size`)를 가지며, `C`개의 카테고리(`num_classes`)에 대한 분류 scores를 예측합니다.  \n",
    "따라서 학습 가능한 **weights**와 **biases**의 shape는 다음과 같습니다:\n",
    "\n",
    "* W1: 첫 번째 layer의 weights; shape (D, H)\n",
    "* b1: 첫 번째 layer의 biases; shape (H,)\n",
    "* W2: 두 번째 layer의 weights; shape (H, C)\n",
    "* b2: 두 번째 layer의 biases; shape (C,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZLdCF3B-AOVT"
   },
   "source": [
    "### 구현 하기: Forward pass (compute scores)\n",
    "Linear Classifiers와 마찬가지로, model weights, 이미지와 레이블의 배치를 입력으로 받아 loss와 각 파라미터에 대한 gradient를 반환하는 함수를 작성합니다.<br>\n",
    "하지만 한 번에 전부 구현하지 않고 여기서는 먼저 **forward pass** 함수만 작성합니다.\n",
    "\n",
    "먼저 `nn_forward_pass`에서 **weights**와 **biases**를 사용해 모든 입력에 대한 **scores**를 계산하는 **forward pass**를 구현하세요.<br>\n",
    "scores를 계산한 뒤, 정답과 비교해 보세요. 두 값의 차이는 `1e-10`보다 작아야 합니다.\n",
    "\n",
    "---\n",
    "\n",
    "**주의 사항**: 난수 생성기가 CPU와 GPU에서 다르게 작동되기 때문에 아래 score 검증은 GPU에서 실행되는 결과와 호환되지 않습니다.\n",
    "만약 GPU에서 실행시키는 경우 `correct_scores`를 아래와 같이 변경하고 실행하세요.\n",
    "\n",
    "```\n",
    "correct_scores = torch.tensor([\n",
    "        [ 9.7003e-08, -1.1143e-07, -3.9961e-08],\n",
    "        [-7.4297e-08,  1.1502e-07,  1.5685e-07],\n",
    "        [-2.5860e-07,  2.2765e-07,  3.2453e-07],\n",
    "        [-4.7257e-07,  9.0935e-07,  4.0368e-07],\n",
    "        [-1.8395e-07,  7.9303e-08,  6.0360e-07]], dtype=torch.float32, device=scores.device)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "tZV9_3ZWAOVU",
    "outputId": "7504b688-c002-4676-c064-29adc38f88a4"
   },
   "outputs": [],
   "source": [
    "from utils import get_toy_data, reset_seed\n",
    "from two_layer_net import nn_forward_pass\n",
    "\n",
    "reset_seed(0)\n",
    "toy_X, toy_y, params = get_toy_data(device=\"cpu\")\n",
    "\n",
    "# TODO: Implement the score computation part of nn_forward_pass\n",
    "scores, _ = nn_forward_pass(params, toy_X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print(scores.dtype)\n",
    "print()\n",
    "print('correct scores:')\n",
    "correct_scores = torch.tensor([\n",
    "        [ 4.5398e-07, -1.9270e-07, -4.8966e-07],\n",
    "        [ 5.4361e-07,  9.8901e-08,  7.2228e-07],\n",
    "        [-5.9020e-08,  2.0401e-09,  1.6904e-07],\n",
    "        [ 2.2207e-07,  5.2379e-08,  2.7437e-07],\n",
    "        [ 3.0348e-07, -5.1789e-08, -6.1906e-08]], dtype=torch.float32, device=scores.device)\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# The difference should be very small. We get < 1e-10\n",
    "scores_diff = (scores - correct_scores).abs().sum().item()\n",
    "print('Difference between your scores and correct scores: %.2e' % scores_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XNJ3ydEAOVW"
   },
   "source": [
    "### 구현 하기: Forward pass (compute loss)\n",
    "이제 `nn_forward_backward`의 첫 번째 부분을 구현합니다.  \n",
    "여기서는 data loss와 regularization loss를 계산합니다.\n",
    "\n",
    "- **data loss**는 **softmax cross-entropy loss**를 사용합니다.  \n",
    "- **regularization loss**는 `W1`, `W2`에 대해 **L2 regularization**을 적용합니다. `b1`, `b2`에는 regularization을 적용하지 않습니다.\n",
    "\n",
    "아래 코드 셀을 통해 구현된 pass를 확인합니다.\n",
    "제대로 구현했다면 loss의 차이는 `1e-4` 미만이어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "wgG6w2uKAOVX",
    "outputId": "e198ce0f-1f05-431e-e724-240aeaa09b5e"
   },
   "outputs": [],
   "source": [
    "from two_layer_net import nn_forward_backward\n",
    "\n",
    "reset_seed(0)\n",
    "toy_X, toy_y, params = get_toy_data(device=\"cpu\")\n",
    "\n",
    "# TODO: Implement the loss computation part of nn_forward_backward\n",
    "loss, _ = nn_forward_backward(params, toy_X, toy_y, reg=0.05)\n",
    "print('Your loss: ', loss.item())\n",
    "correct_loss = 1.0986121892929077\n",
    "print('Correct loss: ', correct_loss)\n",
    "diff = (correct_loss - loss).item()\n",
    "\n",
    "# should be very small, we get < 1e-4\n",
    "print('Difference: %.4e' % diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vExP-7n3AOVa"
   },
   "source": [
    "### 구현하기: Backward pass\n",
    "이제 `nn_forward_backward` 안에서 전체 네트워크의 backward pass을 구현합니다.<br>`W1`, `b1`, `W2`, `b2`에 대한 loss의 gradient를 계산합니다.  \n",
    "\n",
    "아래 코드 셀은 numeric gradient checking을 통해 backward pass에서 계산한 analytic gradient가 수치적으로 구한 numeric gradient와 일치하는지 확인합니다.\n",
    "모든 파라미터에서 relative error가 `1e-4` 이하로 나와야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "qCEkprvoAOVb",
    "outputId": "3e02d110-e672-4e33-80b8-5d898cfb30ef"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from two_layer_net import nn_forward_backward\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "reg = 0.05\n",
    "toy_X, toy_y, params = get_toy_data(device=\"cpu\", dtype=torch.float64)\n",
    "\n",
    "# TODO: Implement the gradient computation part of nn_forward_backward\n",
    "#       When you implement the gradient computation part, you may need to \n",
    "#       implement the `hidden` output in nn_forward_pass, as well.\n",
    "loss, grads = nn_forward_backward(params, toy_X, toy_y, reg=reg)\n",
    "\n",
    "for param_name, grad in grads.items():\n",
    "  param = params[param_name]\n",
    "  f = lambda w: nn_forward_backward(params, toy_X, toy_y, reg=reg)[0]\n",
    "  grad_numeric = utils.compute_numeric_gradient(f, param)\n",
    "  error = utils.rel_error(grad, grad_numeric)\n",
    "  print('%s max relative error: %e' % (param_name, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LjAUalCBAOVd"
   },
   "source": [
    "### 구현 하기: Train the network\n",
    "이 network는 Softmax 분류기 때와 마찬가지로 SGD로 학습합니다.\n",
    "\n",
    "`nn_train` 함수를 살펴보고, 비어 있는 부분을 채워 training 파트를 구현하세요.  \n",
    "또한 학습 도중 주기적으로 prediction을 수행해 accuracy를 추적하므로, `nn_predict`도 함께 구현해야 합니다.\n",
    "\n",
    "모든 구현을 마친 뒤, 아래 코드를 실행해 toy 데이터에 대해 two-layer network를 학습해 보세요.  \n",
    "최종 training loss는 1.0 근처여야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "Wgw06cLXAOVd",
    "outputId": "be163c99-6590-4354-eafa-93d623bed3a8"
   },
   "outputs": [],
   "source": [
    "from two_layer_net import nn_forward_backward, nn_train, nn_predict\n",
    "\n",
    "reset_seed(0)\n",
    "toy_X, toy_y, params = get_toy_data(device=\"cpu\")\n",
    "\n",
    "stats = nn_train(params, nn_forward_backward, nn_predict, toy_X, toy_y, toy_X, toy_y,\n",
    "                 learning_rate=1e-1, reg=1e-6,\n",
    "                 num_iters=200, verbose=False)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8cPIajWNAOVg"
   },
   "source": [
    "## CIFAR-10 데이터셋에 학습하기\n",
    "이제 구현한 **two-layer network**가 gradient check를 통과했고 toy 데이터에서도 잘 동작하니 실제 데이터셋에 학습해볼 차례입니다.<br>\n",
    "아래 코드 셀은 CIFAR-10 데이터를 불러와 classifier를 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "lYo_XrU3AOVg",
    "outputId": "e0e8ca93-3570-45f4-96ec-d03d91b1148c"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "reset_seed(0)\n",
    "data_dict = utils.preprocess_cifar10(cuda=False, dtype=torch.float64)\n",
    "print('Train data shape: ', data_dict['X_train'].shape)\n",
    "print('Train labels shape: ', data_dict['y_train'].shape)\n",
    "print('Validation data shape: ', data_dict['X_val'].shape)\n",
    "print('Validation labels shape: ', data_dict['y_val'].shape)\n",
    "print('Test data shape: ', data_dict['X_test'].shape)\n",
    "print('Test labels shape: ', data_dict['y_test'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cq-HkgRBAOVQ"
   },
   "source": [
    "### Network를 객체 지향적으로 (class) 관리하기\n",
    "사람마다 구현 스타일이 다르지만 network를 class로 관리하는게 편리합니다.<br>\n",
    "코드에서는 network를 표현하기 위해 `TwoLayerNet` class로 구현되었습니다.\n",
    "\n",
    "네트워크의 파라미터들은 인스턴스 변수 `self.params`에 저장됩니다.<br>\n",
    "여기서 key는 문자열 형태의 파라미터 이름이고, value는 PyTorch `tensor`입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_CsYAv3uAOVi"
   },
   "source": [
    "### Network 학습하기\n",
    "\n",
    "모델을 SGD로 학습합니다. 참고로 여기서는 learning rate를 exponential 형태로 조정하는데 추후 수업 때 배울 내용이니 넘어가도 좋습니다.\n",
    "<br>\n",
    "Linear classifier 때와 마찬가지로 아래 코드는 대략 validation accuracy가 10% 내외로 나오게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "hgg0QV9DAOVj",
    "outputId": "ac949f3a-edf9-4a54-c47a-4c348c6c89e2"
   },
   "outputs": [],
   "source": [
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "input_size = 3 * 32 * 32\n",
    "hidden_size = 36\n",
    "num_classes = 10\n",
    "\n",
    "# fix random seed before we generate a set of parameters\n",
    "reset_seed(0)\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes, dtype=data_dict['X_train'].dtype, device=data_dict['X_train'].device)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(data_dict['X_train'], data_dict['y_train'],\n",
    "                  data_dict['X_val'], data_dict['y_val'],\n",
    "                  num_iters=500, batch_size=1000,\n",
    "                  learning_rate=1e-2, learning_rate_decay=0.95,\n",
    "                  reg=0.25, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = net.predict(data_dict['X_val'])\n",
    "val_acc = 100.0 * (y_val_pred == data_dict['y_val']).double().mean().item()\n",
    "print('Validation accuracy: %.2f%%' % val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 loss와 classification accuracy를 시각화합니다.<br>\n",
    "그리고 network의 weight가 어떻게 학습되었는지 vision 관점에서도 보여줍니다.\n",
    "\n",
    "학습이 잘 되지 않았으니 모두 엉망으로 결과가 나오는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "colab_type": "code",
    "id": "6sYXImDTAOVm",
    "outputId": "54a06634-bf14-4bac-e522-b96800b6ae4b"
   },
   "outputs": [],
   "source": [
    "from utils import plot_stats, show_net_weights\n",
    "\n",
    "plot_stats(stats)\n",
    "show_net_weights(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OlVbXxmPNzPY"
   },
   "source": [
    "## 모델 개선하기\n",
    "\n",
    "위 loss history 시각화를 보면 loss가 거의 선형적으로 (너무 천천히) 감소하는 걸 볼 수 있습니다. 이는 learning rate가 너무 낮을 수 있기 때문에 그럴 수 있습니다.  \n",
    "또한 training accuracy와 validation accuracy 사이에 간극이 거의 없는데 이는 모델의 capacity (능력 or 용량)가 낮아서 모델 크기를 키울 필요가 있음을 의미합니다.  \n",
    "\n",
    "반대로 모델을 지나치게 크게 만들면 overfitting이 발생하기 쉽고, 그 결과 training accuracy와 validation accuracy 사이의 간극이 크게 벌어지는 현상을 보게됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rDNZ8ZAnN7hj"
   },
   "source": [
    "### Capacity?\n",
    "초기 모델은 training set과 validation set에서 매우 비슷한 성능을 보입니다.  \n",
    "이는 모델이 underfitting 상태임을 의미하며, capacity를 늘리면 성능이 향상될 수 있다는 뜻입니다.  \n",
    "\n",
    "Neural network의 capacity를 키우는 한 가지 방법은 hidden layer의 크기를 늘리는 것입니다.  \n",
    "여기서는 hidden layer 크기를 키웠을 때 성능이 어떻게 달라지는지 확인합니다.  \n",
    "일반적으로 hidden layer의 크기가 커질수록 validation accuracy가 높아지지만, 너무 커지면 성능 향상이 점차 줄어드는 diminishing returns 현상이 나타날 수 있습니다.\n",
    "\n",
    "참고로 아래 실험부터는 다양한 하이퍼파라미터 (hidden layer 등등)에 대해 실험을 하기 때문에 CPU에서 실행하는 본 과제에서는 시간이 오래 소요될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "colab_type": "code",
    "id": "_C-ChHUlN68f",
    "outputId": "c5fd64d9-1416-473d-b57d-877fc13f7cd6"
   },
   "outputs": [],
   "source": [
    "from utils import plot_acc_curves\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "hidden_sizes = [2, 8, 32, 128] \n",
    "lr = 0.1\n",
    "reg = 0.001\n",
    "\n",
    "stat_dict = {}\n",
    "for hs in hidden_sizes:\n",
    "  print('train with hidden size: {}'.format(hs))\n",
    "  # fix random seed before we generate a set of parameters\n",
    "  reset_seed(0)\n",
    "  net = TwoLayerNet(3 * 32 * 32, hs, 10, device=data_dict['X_train'].device, dtype=data_dict['X_train'].dtype)\n",
    "  stats = net.train(data_dict['X_train'], data_dict['y_train'], data_dict['X_val'], data_dict['y_val'],\n",
    "            num_iters=3000, batch_size=1000,\n",
    "            learning_rate=lr, learning_rate_decay=0.95,\n",
    "            reg=reg, verbose=False)\n",
    "  stat_dict[hs] = stats\n",
    "\n",
    "plot_acc_curves(stat_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpSrK3olUfOZ"
   },
   "source": [
    "### Regularization?\n",
    "Train accuracy와 validation accuracy의 차이가 작았던 또 다른 이유는 regularization일 수 있습니다.  \n",
    "특히 regularization coefficient가 너무 크면 모델이 training data를 제대로 학습하지 못할 수 있습니다.  \n",
    "\n",
    "이 현상을 확인하기 위해 다른 hyperparameter는 고정하고 regularization strength를 바꿔가며 여러 모델을 학습시켜 볼 수 있습니다.  \n",
    "\n",
    "regularization strength가 지나치게 크면 validation set에서의 모델 성능이 오히려 떨어지는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "id": "DRPsxxFnU3Un",
    "outputId": "2c631a91-a69a-4b62-e36b-29c829536b37"
   },
   "outputs": [],
   "source": [
    "from utils import plot_acc_curves\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "hs = 128\n",
    "lr = 1.0\n",
    "regs = [0, 1e-5, 1e-3, 1e-1]\n",
    "\n",
    "stat_dict = {}\n",
    "for reg in regs:\n",
    "  print('train with regularization: {}'.format(reg))\n",
    "  # fix random seed before we generate a set of parameters\n",
    "  reset_seed(0)\n",
    "  net = TwoLayerNet(3 * 32 * 32, hs, 10, device=data_dict['X_train'].device, dtype=data_dict['X_train'].dtype)\n",
    "  stats = net.train(data_dict['X_train'], data_dict['y_train'], data_dict['X_val'], data_dict['y_val'],\n",
    "            num_iters=3000, batch_size=1000,\n",
    "            learning_rate=lr, learning_rate_decay=0.95,\n",
    "            reg=reg, verbose=False)\n",
    "  stat_dict[reg] = stats\n",
    "\n",
    "plot_acc_curves(stat_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zFWkxebWXtu"
   },
   "source": [
    "### Learning Rate?\n",
    "물론 learning rate도 매우 중요한 하이퍼파라미터 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "id": "lc_YYCDmWld-",
    "outputId": "19367e7d-0c8e-47fb-e895-a6e6b38934e6"
   },
   "outputs": [],
   "source": [
    "from utils import plot_acc_curves\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "hs = 128\n",
    "lrs = [1e-4, 1e-2, 1e0, 1e2]\n",
    "reg = 1e-4\n",
    "\n",
    "stat_dict = {}\n",
    "for lr in lrs:\n",
    "  print('train with learning rate: {}'.format(lr))\n",
    "  # fix random seed before we generate a set of parameters\n",
    "  reset_seed(0)\n",
    "  net = TwoLayerNet(3 * 32 * 32, hs, 10, device=data_dict['X_train'].device, dtype=data_dict['X_train'].dtype)\n",
    "  stats = net.train(data_dict['X_train'], data_dict['y_train'], data_dict['X_val'], data_dict['y_val'],\n",
    "            num_iters=3000, batch_size=1000,\n",
    "            learning_rate=lr, learning_rate_decay=0.95,\n",
    "            reg=reg, verbose=False)\n",
    "  stat_dict[lr] = stats\n",
    "\n",
    "plot_acc_curves(stat_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGbRHnoEAUVN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eYE9thuXn4zP",
    "CdowvtJen-IP",
    "KtMy3qeipNK3",
    "Hbe3wUpVAjma",
    "lJqim3P1qZgv",
    "ZLdCF3B-AOVT",
    "7XNJ3ydEAOVW",
    "vExP-7n3AOVa",
    "LjAUalCBAOVd",
    "8cPIajWNAOVg",
    "_CsYAv3uAOVi",
    "ixxgq5RKAOVl",
    "OlVbXxmPNzPY",
    "rDNZ8ZAnN7hj",
    "QpSrK3olUfOZ",
    "3zFWkxebWXtu",
    "mVCEro4FAOVq",
    "UG56gKWsAOVv",
    "37R_J2uMP3d-"
   ],
   "name": "two_layer_net.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
